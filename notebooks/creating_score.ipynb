{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTwTGeAlSQT57xyf9DK4Ff",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kgreed4/parkisons_exploration/blob/main/progression_score/creating_score.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import torch\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "euNmc_nxZLil"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torchsummary import summary\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "0ECO6EuRibhK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite"
      ],
      "metadata": {
        "id": "S9S0AvnLapwd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# Define the transformation to be applied to the input image\n",
        "# Apply any transformations here\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load your input image\n",
        "input_image = Image.open(\"/content/Screenshot 2024-04-19 at 11.51.34 AM.png\")  # Replace \"path_to_your_image.jpg\" with the path to your image\n",
        "input_image = input_image.convert(\"RGB\")\n",
        "input_image = transform(input_image)  # Apply the transformation to the input image\n",
        "input_image = input_image.unsqueeze(0)  # Add a batch dimension\n",
        "\n",
        "# Define RESNET50 model\n",
        "handwriting_model = models.resnet50(weights=True)\n",
        "\n",
        "# Freeze model parameters\n",
        "for param in handwriting_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify pooling layer\n",
        "handwriting_model.avgpool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
        "\n",
        "# Replace last FC layer\n",
        "handwriting_model.fc = nn.Sequential(nn.Flatten(),\n",
        "                          nn.Linear(2048, 256),\n",
        "                          nn.ReLU(),\n",
        "                          nn.Dropout(0.2),\n",
        "                          nn.Linear(256, 2))\n",
        "\n",
        "# Load the saved state_dict\n",
        "state_dict = torch.load('/content/best_resNet50_w_weights_100.pth', map_location=torch.device('cpu'))\n",
        "\n",
        "# Load the state_dict into the model\n",
        "handwriting_model.load_state_dict(state_dict)\n",
        "\n",
        "# Put the model in evaluation mode\n",
        "handwriting_model.eval()\n",
        "\n",
        "# Make prediction\n",
        "with torch.no_grad():\n",
        "    outputs = handwriting_model(input_image)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    softmax_probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "\n",
        "predicted_label = predicted.item()  # Convert predicted label tensor to Python scalar\n",
        "handwriting_predictions = softmax_probs.numpy().squeeze()  # Convert prediction probabilities tensor to numpy array\n",
        "\n",
        "print(\"Predicted Label:\", predicted_label)\n",
        "print(\"Prediction Probabilities:\", handwriting_predictions)\n",
        "\n",
        "# FOR HANDWRITING, healthy = 0, PD = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgMBRAtRj1yv",
        "outputId": "c67635c0-fde6-443b-901f-0d10a66b0a0a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Label: 0\n",
            "Prediction Probabilities: [0.97484004 0.02515993]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "\n",
        "# Load the audio file\n",
        "def load_audio(audio_path, sr=None):\n",
        "    y, sr = sf.read(audio_path)\n",
        "    return y, sr\n",
        "\n",
        "# Extract features from audio\n",
        "def extract_features(y, sr):\n",
        "    features = []\n",
        "\n",
        "    # Extract features using librosa\n",
        "    # Fundamental frequency features\n",
        "    f0, voiced_flag = librosa.piptrack(y=y, sr=sr)\n",
        "    features.append(np.mean(f0))  # MDVP:Fo(Hz)\n",
        "    features.append(np.max(f0))  # MDVP:Fhi(Hz)\n",
        "    features.append(np.min(f0))  # MDVP:Flo(Hz)\n",
        "\n",
        "    # Jitter features\n",
        "    jitter = librosa.effects.split(y, top_db=20)\n",
        "    features.append(librosa.feature.rms(y=y, frame_length=20, hop_length=10).mean())  # MDVP:Jitter(%)\n",
        "    features.append(np.mean(np.abs(np.diff(f0))))  # MDVP:Jitter(Abs)\n",
        "    features.append(np.mean(np.diff(f0)))  # MDVP:RAP\n",
        "    features.append(np.mean(np.abs(np.diff(np.diff(f0)))))  # MDVP:PPQ\n",
        "    features.append(np.mean(np.abs(np.diff(f0))) * 3)  # Jitter:DDP\n",
        "\n",
        "    # Shimmer features\n",
        "    shimmer = librosa.effects.split(y, top_db=40)\n",
        "    if shimmer.size > 0:\n",
        "        shimmer_rms = [librosa.feature.rms(y=y[start:end], frame_length=20, hop_length=10).mean() for start, end in shimmer]\n",
        "        features.append(np.mean(shimmer_rms))  # MDVP:Shimmer\n",
        "        # Compute shimmer in dB\n",
        "        shimmer_db = [np.mean(librosa.amplitude_to_db(librosa.feature.rms(y=y[start:end], frame_length=20, hop_length=10))) for start, end in shimmer]\n",
        "        features.append(np.mean(shimmer_db))  # MDVP:Shimmer(dB)\n",
        "        # Compute the mean second-order difference of the signal amplitude as a proxy for sharpness\n",
        "        sharpness = [np.mean(np.diff(np.abs(np.diff(y[start:end])))) for start, end in shimmer]\n",
        "        features.append(np.mean(sharpness))  # Shimmer:APQ\n",
        "        features.append(np.mean(sharpness))  # Shimmer:APQ3\n",
        "        features.append(np.mean(sharpness))  # Shimmer:APQ5\n",
        "    else:\n",
        "        features.extend([0] * 3)  # If shimmer is empty, set to 0 for all shimmer-related features\n",
        "\n",
        "    # Other features\n",
        "    features.append(np.mean(librosa.effects.split(y, top_db=60)))  # Shimmer:DDA\n",
        "    features.append(len(jitter))  # NHR\n",
        "    features.append(np.mean(librosa.effects.split(y, top_db=60)))  # RPDE\n",
        "    features.append(np.mean(librosa.effects.split(y, top_db=60)))  # DFA\n",
        "    features.append(np.mean(librosa.effects.split(y, top_db=60)))  # spread1\n",
        "    features.append(np.mean(librosa.effects.split(y, top_db=60)))  # spread2\n",
        "    features.append(np.mean(librosa.effects.split(y, top_db=60)))  # D2\n",
        "    features.append(np.mean(librosa.effects.split(y, top_db=60)))  # PPE\n",
        "    features.append(np.mean(librosa.effects.split(y, top_db=60)))  # HNR\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def get_voice_memo():\n",
        "  !ffmpeg -i '/content/The Joyce Center 3.m4a' -acodec libmp3lame -q:a 2 output.mp3\n",
        "  # Load audio\n",
        "  audio_path = '/content/output.mp3'\n",
        "\n",
        "  y, sr = load_audio(audio_path)\n",
        "  features = extract_features(y, sr)\n",
        "\n",
        "  return features\n"
      ],
      "metadata": {
        "id": "2fQ06AYvVpdG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQafShq8hWIl",
        "outputId": "939b38e7-337d-484a-e1f2-0aff255ba6f1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_docs\n",
            "  Downloading tensorflow_docs-2024.2.5.73858-py3-none-any.whl (182 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/182.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m143.4/182.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.5/182.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting astor (from tensorflow_docs)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow_docs) (1.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from tensorflow_docs) (3.1.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from tensorflow_docs) (5.10.4)\n",
            "Requirement already satisfied: protobuf>=3.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow_docs) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from tensorflow_docs) (6.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->tensorflow_docs) (2.1.5)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->tensorflow_docs) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->tensorflow_docs) (4.19.2)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat->tensorflow_docs) (5.7.2)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbformat->tensorflow_docs) (5.7.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->tensorflow_docs) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->tensorflow_docs) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->tensorflow_docs) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->tensorflow_docs) (0.18.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->tensorflow_docs) (4.2.0)\n",
            "Installing collected packages: astor, tensorflow_docs\n",
            "Successfully installed astor-0.8.1 tensorflow_docs-2024.2.5.73858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_io"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGvr24TIhkuQ",
        "outputId": "32b3a9ae-c9ed-41fc-9e3b-074d0a1e543e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_io\n",
            "  Downloading tensorflow_io-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem==0.36.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_io) (0.36.0)\n",
            "Installing collected packages: tensorflow_io\n",
            "Successfully installed tensorflow_io-0.36.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_docs.vis import embed\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.collections import LineCollection\n",
        "import matplotlib.patches as patches\n",
        "import tensorflow_io as tfio\n",
        "import subprocess\n",
        "from datetime import timedelta\n",
        "\n",
        "# Dictionary that maps from joint names to keypoint indices.\n",
        "KEYPOINT_DICT = {\n",
        "    'nose': 0,\n",
        "    'left_eye': 1,\n",
        "    'right_eye': 2,\n",
        "    'left_ear': 3,\n",
        "    'right_ear': 4,\n",
        "    'left_shoulder': 5,\n",
        "    'right_shoulder': 6,\n",
        "    'left_elbow': 7,\n",
        "    'right_elbow': 8,\n",
        "    'left_wrist': 9,\n",
        "    'right_wrist': 10,\n",
        "    'left_hip': 11,\n",
        "    'right_hip': 12,\n",
        "    'left_knee': 13,\n",
        "    'right_knee': 14,\n",
        "    'left_ankle': 15,\n",
        "    'right_ankle': 16\n",
        "}\n",
        "\n",
        "# Confidence score to determine whether a keypoint prediction is reliable.\n",
        "MIN_CROP_KEYPOINT_SCORE = 0.2\n",
        "\n",
        "# Helper Functions\n",
        "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
        "                                     height,\n",
        "                                     width,\n",
        "                                     keypoint_threshold=0.11):\n",
        "  \"\"\"Returns high confidence keypoints and edges for visualization.\n",
        "\n",
        "  Args:\n",
        "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
        "      the keypoint coordinates and scores returned from the MoveNet model.\n",
        "    height: height of the image in pixels.\n",
        "    width: width of the image in pixels.\n",
        "    keypoint_threshold: minimum confidence score for a keypoint to be\n",
        "      visualized.\n",
        "\n",
        "  Returns:\n",
        "    A (keypoints_xy, edges_xy, edge_colors) containing:\n",
        "      * the coordinates of all keypoints of all detected entities;\n",
        "      * the coordinates of all skeleton edges of all detected entities;\n",
        "      * the colors in which the edges should be plotted.\n",
        "  \"\"\"\n",
        "  keypoints_all = []\n",
        "  keypoint_edges_all = []\n",
        "  edge_colors = []\n",
        "  num_instances, _, _, _ = keypoints_with_scores.shape\n",
        "  for idx in range(num_instances):\n",
        "    kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
        "    kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
        "    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
        "    kpts_absolute_xy = np.stack(\n",
        "        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
        "    kpts_above_thresh_absolute = kpts_absolute_xy[\n",
        "        kpts_scores > keypoint_threshold, :]\n",
        "    keypoints_all.append(kpts_above_thresh_absolute)\n",
        "\n",
        "    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
        "      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
        "          kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
        "        x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
        "        y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
        "        x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
        "        y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
        "        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
        "        keypoint_edges_all.append(line_seg)\n",
        "        edge_colors.append(color)\n",
        "  if keypoints_all:\n",
        "    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
        "  else:\n",
        "    keypoints_xy = np.zeros((0, 17, 2))\n",
        "\n",
        "  if keypoint_edges_all:\n",
        "    edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
        "  else:\n",
        "    edges_xy = np.zeros((0, 2, 2))\n",
        "  return keypoints_xy, edges_xy, edge_colors\n",
        "\n",
        "\n",
        "def draw_prediction_on_image(\n",
        "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
        "    output_image_height=None):\n",
        "  \"\"\"Draws the keypoint predictions on image.\n",
        "\n",
        "  Args:\n",
        "    image: A numpy array with shape [height, width, channel] representing the\n",
        "      pixel values of the input image.\n",
        "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
        "      the keypoint coordinates and scores returned from the MoveNet model.\n",
        "    crop_region: A dictionary that defines the coordinates of the bounding box\n",
        "      of the crop region in normalized coordinates (see the init_crop_region\n",
        "      function below for more detail). If provided, this function will also\n",
        "      draw the bounding box on the image.\n",
        "    output_image_height: An integer indicating the height of the output image.\n",
        "      Note that the image aspect ratio will be the same as the input image.\n",
        "\n",
        "  Returns:\n",
        "    A numpy array with shape [out_height, out_width, channel] representing the\n",
        "    image overlaid with keypoint predictions.\n",
        "  \"\"\"\n",
        "  height, width, channel = image.shape\n",
        "  aspect_ratio = float(width) / height\n",
        "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
        "  # To remove the huge white borders\n",
        "  fig.tight_layout(pad=0)\n",
        "  ax.margins(0)\n",
        "  ax.set_yticklabels([])\n",
        "  ax.set_xticklabels([])\n",
        "  plt.axis('off')\n",
        "\n",
        "  im = ax.imshow(image)\n",
        "  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
        "  ax.add_collection(line_segments)\n",
        "  # Turn off tick labels\n",
        "  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
        "\n",
        "  (keypoint_locs, keypoint_edges,\n",
        "   edge_colors) = _keypoints_and_edges_for_display(\n",
        "       keypoints_with_scores, height, width)\n",
        "\n",
        "  line_segments.set_segments(keypoint_edges)\n",
        "  line_segments.set_color(edge_colors)\n",
        "  if keypoint_edges.shape[0]:\n",
        "    line_segments.set_segments(keypoint_edges)\n",
        "    line_segments.set_color(edge_colors)\n",
        "  if keypoint_locs.shape[0]:\n",
        "    scat.set_offsets(keypoint_locs)\n",
        "\n",
        "  if crop_region is not None:\n",
        "    xmin = max(crop_region['x_min'] * width, 0.0)\n",
        "    ymin = max(crop_region['y_min'] * height, 0.0)\n",
        "    rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
        "    rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
        "    rect = patches.Rectangle(\n",
        "        (xmin,ymin),rec_width,rec_height,\n",
        "        linewidth=1,edgecolor='b',facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "  fig.canvas.draw()\n",
        "  image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "  image_from_plot = image_from_plot.reshape(\n",
        "      fig.canvas.get_width_height()[::-1] + (3,))\n",
        "  plt.close(fig)\n",
        "  if output_image_height is not None:\n",
        "    output_image_width = int(output_image_height / height * width)\n",
        "    image_from_plot = cv2.resize(\n",
        "        image_from_plot, dsize=(output_image_width, output_image_height),\n",
        "         interpolation=cv2.INTER_CUBIC)\n",
        "  return image_from_plot\n",
        "\n",
        "def progress(value, max=100):\n",
        "  return HTML(\"\"\"\n",
        "      <progress\n",
        "          value='{value}'\n",
        "          max='{max}',\n",
        "          style='width: 100%'\n",
        "      >\n",
        "          {value}\n",
        "      </progress>\n",
        "  \"\"\".format(value=value, max=max))\n",
        "\n",
        "def movenet(input_image):\n",
        "    \"\"\"Runs detection on an input image.\n",
        "\n",
        "    Args:\n",
        "      input_image: A [1, height, width, 3] tensor represents the input image\n",
        "        pixels. Note that the height/width should already be resized and match the\n",
        "        expected input resolution of the model before passing into this function.\n",
        "\n",
        "    Returns:\n",
        "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
        "      coordinates and scores.\n",
        "    \"\"\"\n",
        "    # TF Lite format expects tensor type of uint8.\n",
        "    input_image = tf.cast(input_image, dtype=tf.uint8)\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
        "    # Invoke inference.\n",
        "    interpreter.invoke()\n",
        "    # Get the model prediction.\n",
        "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
        "    return keypoints_with_scores\n",
        "\n",
        "# Cropping Algorithm\n",
        "def init_crop_region(image_height, image_width):\n",
        "  \"\"\"Defines the default crop region.\n",
        "\n",
        "  The function provides the initial crop region (pads the full image from both\n",
        "  sides to make it a square image) when the algorithm cannot reliably determine\n",
        "  the crop region from the previous frame.\n",
        "  \"\"\"\n",
        "  if image_width > image_height:\n",
        "    box_height = image_width / image_height\n",
        "    box_width = 1.0\n",
        "    y_min = (image_height / 2 - image_width / 2) / image_height\n",
        "    x_min = 0.0\n",
        "  else:\n",
        "    box_height = 1.0\n",
        "    box_width = image_height / image_width\n",
        "    y_min = 0.0\n",
        "    x_min = (image_width / 2 - image_height / 2) / image_width\n",
        "\n",
        "  return {\n",
        "    'y_min': y_min,\n",
        "    'x_min': x_min,\n",
        "    'y_max': y_min + box_height,\n",
        "    'x_max': x_min + box_width,\n",
        "    'height': box_height,\n",
        "    'width': box_width\n",
        "  }\n",
        "\n",
        "def torso_visible(keypoints):\n",
        "  \"\"\"Checks whether there are enough torso keypoints.\n",
        "\n",
        "  This function checks whether the model is confident at predicting one of the\n",
        "  shoulders/hips which is required to determine a good crop region.\n",
        "  \"\"\"\n",
        "  return ((keypoints[0, 0, KEYPOINT_DICT['left_hip'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE or\n",
        "          keypoints[0, 0, KEYPOINT_DICT['right_hip'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE) and\n",
        "          (keypoints[0, 0, KEYPOINT_DICT['left_shoulder'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE or\n",
        "          keypoints[0, 0, KEYPOINT_DICT['right_shoulder'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE))\n",
        "\n",
        "def determine_torso_and_body_range(\n",
        "    keypoints, target_keypoints, center_y, center_x):\n",
        "  \"\"\"Calculates the maximum distance from each keypoints to the center location.\n",
        "\n",
        "  The function returns the maximum distances from the two sets of keypoints:\n",
        "  full 17 keypoints and 4 torso keypoints. The returned information will be\n",
        "  used to determine the crop size. See determineCropRegion for more detail.\n",
        "  \"\"\"\n",
        "  torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
        "  max_torso_yrange = 0.0\n",
        "  max_torso_xrange = 0.0\n",
        "  for joint in torso_joints:\n",
        "    dist_y = abs(center_y - target_keypoints[joint][0])\n",
        "    dist_x = abs(center_x - target_keypoints[joint][1])\n",
        "    if dist_y > max_torso_yrange:\n",
        "      max_torso_yrange = dist_y\n",
        "    if dist_x > max_torso_xrange:\n",
        "      max_torso_xrange = dist_x\n",
        "\n",
        "  max_body_yrange = 0.0\n",
        "  max_body_xrange = 0.0\n",
        "  for joint in KEYPOINT_DICT.keys():\n",
        "    if keypoints[0, 0, KEYPOINT_DICT[joint], 2] < MIN_CROP_KEYPOINT_SCORE:\n",
        "      continue\n",
        "    dist_y = abs(center_y - target_keypoints[joint][0]);\n",
        "    dist_x = abs(center_x - target_keypoints[joint][1]);\n",
        "    if dist_y > max_body_yrange:\n",
        "      max_body_yrange = dist_y\n",
        "\n",
        "    if dist_x > max_body_xrange:\n",
        "      max_body_xrange = dist_x\n",
        "\n",
        "  return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]\n",
        "\n",
        "def determine_crop_region(\n",
        "      keypoints, image_height,\n",
        "      image_width):\n",
        "  \"\"\"Determines the region to crop the image for the model to run inference on.\n",
        "\n",
        "  The algorithm uses the detected joints from the previous frame to estimate\n",
        "  the square region that encloses the full body of the target person and\n",
        "  centers at the midpoint of two hip joints. The crop size is determined by\n",
        "  the distances between each joints and the center point.\n",
        "  When the model is not confident with the four torso joint predictions, the\n",
        "  function returns a default crop which is the full image padded to square.\n",
        "  \"\"\"\n",
        "  target_keypoints = {}\n",
        "  for joint in KEYPOINT_DICT.keys():\n",
        "    target_keypoints[joint] = [\n",
        "      keypoints[0, 0, KEYPOINT_DICT[joint], 0] * image_height,\n",
        "      keypoints[0, 0, KEYPOINT_DICT[joint], 1] * image_width\n",
        "    ]\n",
        "\n",
        "  if torso_visible(keypoints):\n",
        "    center_y = (target_keypoints['left_hip'][0] +\n",
        "                target_keypoints['right_hip'][0]) / 2;\n",
        "    center_x = (target_keypoints['left_hip'][1] +\n",
        "                target_keypoints['right_hip'][1]) / 2;\n",
        "\n",
        "    (max_torso_yrange, max_torso_xrange,\n",
        "      max_body_yrange, max_body_xrange) = determine_torso_and_body_range(\n",
        "          keypoints, target_keypoints, center_y, center_x)\n",
        "\n",
        "    crop_length_half = np.amax(\n",
        "        [max_torso_xrange * 1.9, max_torso_yrange * 1.9,\n",
        "          max_body_yrange * 1.2, max_body_xrange * 1.2])\n",
        "\n",
        "    tmp = np.array(\n",
        "        [center_x, image_width - center_x, center_y, image_height - center_y])\n",
        "    crop_length_half = np.amin(\n",
        "        [crop_length_half, np.amax(tmp)]);\n",
        "\n",
        "    crop_corner = [center_y - crop_length_half, center_x - crop_length_half];\n",
        "\n",
        "    if crop_length_half > max(image_width, image_height) / 2:\n",
        "      return init_crop_region(image_height, image_width)\n",
        "    else:\n",
        "      crop_length = crop_length_half * 2;\n",
        "      return {\n",
        "        'y_min': crop_corner[0] / image_height,\n",
        "        'x_min': crop_corner[1] / image_width,\n",
        "        'y_max': (crop_corner[0] + crop_length) / image_height,\n",
        "        'x_max': (crop_corner[1] + crop_length) / image_width,\n",
        "        'height': (crop_corner[0] + crop_length) / image_height -\n",
        "            crop_corner[0] / image_height,\n",
        "        'width': (crop_corner[1] + crop_length) / image_width -\n",
        "            crop_corner[1] / image_width\n",
        "      }\n",
        "  else:\n",
        "    return init_crop_region(image_height, image_width)\n",
        "\n",
        "def crop_and_resize(image, crop_region, crop_size):\n",
        "  \"\"\"Crops and resize the image to prepare for the model input.\"\"\"\n",
        "  boxes=[[crop_region['y_min'], crop_region['x_min'],\n",
        "          crop_region['y_max'], crop_region['x_max']]]\n",
        "  output_image = tf.image.crop_and_resize(\n",
        "      image, box_indices=[0], boxes=boxes, crop_size=crop_size)\n",
        "  return output_image\n",
        "\n",
        "def run_inference(movenet, image, crop_region, crop_size):\n",
        "  \"\"\"Runs model inference on the cropped region.\n",
        "\n",
        "  The function runs the model inference on the cropped region and updates the\n",
        "  model output to the original image coordinate system.\n",
        "  \"\"\"\n",
        "  image_height, image_width, _ = image.shape\n",
        "  input_image = crop_and_resize(\n",
        "    tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size)\n",
        "  # Run model inference.\n",
        "  keypoints_with_scores = movenet(input_image)\n",
        "  # Update the coordinates.\n",
        "  for idx in range(17):\n",
        "    keypoints_with_scores[0, 0, idx, 0] = (\n",
        "        crop_region['y_min'] * image_height +\n",
        "        crop_region['height'] * image_height *\n",
        "        keypoints_with_scores[0, 0, idx, 0]) / image_height\n",
        "    keypoints_with_scores[0, 0, idx, 1] = (\n",
        "        crop_region['x_min'] * image_width +\n",
        "        crop_region['width'] * image_width *\n",
        "        keypoints_with_scores[0, 0, idx, 1]) / image_width\n",
        "  return keypoints_with_scores\n",
        "\n",
        "def extract_video_data(image, video_path, input_size):\n",
        "\n",
        "    label_list = []\n",
        "    label_list = ['frame #', 'time']\n",
        "\n",
        "    # Create x and y values for each pose estimate point\n",
        "    for i in KEYPOINT_DICT.keys():\n",
        "        label_list.append(i+'_x')\n",
        "        label_list.append(i+'_y')\n",
        "\n",
        "    # Create df to store output for each point at each frame\n",
        "    df = pd.DataFrame(columns=label_list)\n",
        "\n",
        "    # Load the input image.\n",
        "    num_frames, image_height, image_width, _ = image.shape\n",
        "    crop_region = init_crop_region(image_height, image_width)\n",
        "\n",
        "    # Run ffprobe to get video metadata\n",
        "    ffprobe_command = [\n",
        "        'ffprobe', '-v', 'error', '-select_streams', 'v:0', '-show_entries',\n",
        "        'stream=avg_frame_rate', '-of', 'default=noprint_wrappers=1:nokey=1',\n",
        "        video_path\n",
        "    ]\n",
        "    video_output = subprocess.check_output(ffprobe_command).decode('utf-8').strip()\n",
        "\n",
        "    # Parse the output to get the frame rate\n",
        "    numerator, denominator = map(int, video_output.split('/'))\n",
        "    frame_rate = numerator / denominator\n",
        "\n",
        "    # Calculate the time interval between frames.\n",
        "    time_interval = timedelta(seconds=1 / frame_rate)\n",
        "\n",
        "    # Loop through the frames\n",
        "    for frame_idx in range(num_frames):\n",
        "\n",
        "        # Get keypoints\n",
        "        keypoints_with_scores = run_inference(\n",
        "            movenet, image[frame_idx, :, :, :], crop_region,\n",
        "            crop_size=[input_size, input_size])\n",
        "\n",
        "        # Calculate the timestamp for the current frame.\n",
        "        frame_time = time_interval * frame_idx\n",
        "\n",
        "        # Extract keypoints\n",
        "        keypoints = []\n",
        "        for i in range(17):\n",
        "            keypoints.append(keypoints_with_scores[0, 0, i, 0])\n",
        "            keypoints.append(keypoints_with_scores[0, 0, i, 1])\n",
        "\n",
        "        df.loc[frame_idx] = [frame_idx, frame_time] + keypoints\n",
        "\n",
        "        # Crop the frame to the region of interest.\n",
        "        crop_region = determine_crop_region(\n",
        "            keypoints_with_scores, image_height, image_width)\n",
        "\n",
        "    return df\n",
        "\n",
        "'''\n",
        "This function ges the input from the user and sets the attributes of the output_df.\n",
        "'''\n",
        "def get_user_input(output_df):\n",
        "  # Get input from the user\n",
        "  on_off_medication = input(\"On or Off medication: \") #either On medication, Off medication\n",
        "  dbs_state = input(\"DBS state: \") # If healthy control participant: always \"Control\". If participant with Parkinson's disease: either \"On DBS\" (deep brain stimulator switched on or within 1 hour of it being switched off), \"Off DBS\" (1 hour or longer after deep brain stimulator switched off until it is switched back on again) or \"-\" (no deep brain stimulator in situ).\n",
        "\n",
        "  # Set attributes inputted to output_df\n",
        "  output_df.loc[0, 'On_or_Off_medication'] = on_off_medication\n",
        "  output_df.loc[0, 'DBS_state'] = dbs_state\n",
        "  return output_df\n",
        "\n",
        "'''\n",
        "This function calculates the sts_whole_episode_duration.\n",
        "'''\n",
        "def calculate_sts_whole_episode_duration(df, output_df):\n",
        "  # Get frame 3 time\n",
        "  start_time = df.loc[df['frame #'] == 3, 'time'].iloc[0]\n",
        "\n",
        "  # Find the max frame\n",
        "  max_frame = df['frame #'].max()\n",
        "  end_time = df.loc[df['frame #'] == max_frame-3, 'time'].iloc[0]\n",
        "\n",
        "  sts_whole_episode_duration = end_time - start_time\n",
        "\n",
        "  # Save in first entry of output_df\n",
        "  output_df.loc[0, 'sts_whole_episode_duration'] = sts_whole_episode_duration.total_seconds()\n",
        "\n",
        "  return output_df\n",
        "\n",
        "def calculate_sts_final_attempt_duration(df, output_df):\n",
        "  # Duration in seconds of \"final attempt duration\" label in milliseconds, comprising their impression of the duration between the lowest point of the head (start)\n",
        "  # and when the person was fully upright/the maximum vertical position of the vertex of the head (end).\n",
        "\n",
        "  # Create a head column that is the average y values of nose_y, left_eye_y, right_eye_y, left_ear_y, right_ear_y\n",
        "  df['head'] = (df['nose_y'] + df['left_eye_y'] + df['right_eye_y'] + df['left_ear_y'] + df['right_ear_y']) / 5\n",
        "\n",
        "  # Get min and max values and their rows from head column\n",
        "  # REMEMBER: Pose estimation y-axis starts at top left corner and moves down (reverse)\n",
        "  min_head = df['head'].min()\n",
        "  max_head = df['head'].max()\n",
        "\n",
        "  # Get highest head point\n",
        "  highest_head = df[df['head'] == min_head]\n",
        "\n",
        "  # Get lowest head point\n",
        "  lowest_head = df[df['head'] == max_head]\n",
        "\n",
        "  # Get time entry at highest_head and lowest_head\n",
        "  highest_head_time = highest_head['time'].iloc[0]\n",
        "  lowest_head_time = lowest_head['time'].iloc[0]\n",
        "  print('Highest head time: ', highest_head_time, 'Lowest head time: ', lowest_head_time)\n",
        "\n",
        "  # Calculate final attempt duration\n",
        "  final_attempt_duration = highest_head_time.total_seconds() - lowest_head_time.total_seconds()\n",
        "  print('Total duration: ', final_attempt_duration)\n",
        "\n",
        "  # Save in second entry of output_df\n",
        "  output_df.loc[0, 'sts_final_attempt_duration'] = final_attempt_duration\n",
        "\n",
        "  return output_df\n",
        "\n",
        "def calculate_MDS_UPDRS_score_3_9_and_STS_additional_features(df, output_df):\n",
        "  mds_score = 0\n",
        "  STS_additional_features = []\n",
        "\n",
        "  # Compute the differene between min and max values of left_shoulder_x, right_shoudler_x, right_elbow_x, left_elbow_x, left_hip_x, right_hip_x from df\n",
        "  left_shoulder_x_diff = df['left_shoulder_x'].max() - df['left_shoulder_x'].min()\n",
        "  right_shoulder_x_diff = df['right_shoulder_x'].max() - df['right_shoulder_x'].min()\n",
        "  left_elbow_x_diff = df['left_elbow_x'].max() - df['left_elbow_x'].min()\n",
        "  right_elbow_x_diff = df['right_elbow_x'].max() - df['right_elbow_x'].min()\n",
        "  left_hip_x_diff = df['left_hip_x'].max() - df['left_hip_x'].min()\n",
        "  right_hip_x_diff = df['right_hip_x'].max() - df['right_hip_x'].min()\n",
        "\n",
        "  # Compare differences to threshold value to determine swaying\n",
        "  threshold = 0.25\n",
        "  if left_shoulder_x_diff > threshold or right_shoulder_x_diff > threshold or left_elbow_x_diff > threshold or right_elbow_x_diff > threshold or left_hip_x_diff > threshold or right_hip_x_diff > threshold:\n",
        "      mds_score = 2\n",
        "      STS_additional_features.append(\"Uses arms of chair\")\n",
        "  else:\n",
        "      print(\"No sway detected.\")\n",
        "\n",
        "  # If whole duration > ~5.5 seconds then MDS score = 3\n",
        "  if output_df.loc[0, 'sts_whole_episode_duration'] > 5.5:\n",
        "    mds_score = 3\n",
        "\n",
        "  # If final duration is greater than 2.1 seconds, then STS additioanl features include \"Slow\"\n",
        "  if output_df.loc[0, 'sts_final_attempt_duration'] > 2.1:\n",
        "      STS_additional_features.append(\"Slow\")\n",
        "\n",
        "  # Set mds_score and STS additional features in output_df\n",
        "  output_df.loc[0, 'MDS-UPDRS_score_3.9 _arising_from_chair'] = mds_score\n",
        "  output_df.loc[0, 'STS_additional_features'] = ','.join(STS_additional_features)\n",
        "\n",
        "  return output_df\n",
        "\n",
        "def analyze_video(df):\n",
        "    # Create output dataframe\n",
        "    output_df = pd.DataFrame(columns=['sts_whole_episode_duration','sts_final_attempt_duration','On_or_Off_medication','DBS_state','STS_additional_features','MDS-UPDRS_score_3.9 _arising_from_chair'])\n",
        "\n",
        "    # Get user input\n",
        "    output_df = get_user_input(output_df)\n",
        "\n",
        "    # Calculate sts_whole_episode_duration\n",
        "    output_df = calculate_sts_whole_episode_duration(df, output_df)\n",
        "\n",
        "    # Calculate sts_final_attempt_duration\n",
        "    output_df = calculate_sts_final_attempt_duration(df, output_df)\n",
        "\n",
        "    # Calculate MDS and STS additional features\n",
        "    output_df = calculate_MDS_UPDRS_score_3_9_and_STS_additional_features(df, output_df)\n",
        "\n",
        "    return output_df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    def main():\n",
        "        model_name = \"movenet_lightning_f16.tflite\"\n",
        "        # !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite\n",
        "        input_size = 192\n",
        "\n",
        "        # Initialize the TFLite interpreter\n",
        "        interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
        "        interpreter.allocate_tensors()\n",
        "\n",
        "        # Load and preproces the video\n",
        "        video_path = '/content/IMG_8295.MOV'\n",
        "        video = tf.io.read_file(video_path)\n",
        "        image = tfio.experimental.ffmpeg.decode_video(video)\n",
        "\n",
        "        # Extract video data\n",
        "        df = extract_video_data(image, video_path, input_size)\n",
        "\n",
        "        # Analyze video to obtain data needed for classification model\n",
        "        output_df = analyze_video(df)\n",
        "\n",
        "        return output_df"
      ],
      "metadata": {
        "id": "1T9dD0CzcHEd"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sts_output_df = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQb8315xiIGx",
        "outputId": "c027ca6b-6ffd-46a8-a4fd-a949be7c847a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On or Off medication: Off medication\n",
            "DBS state: -\n",
            "Highest head time:  0 days 00:00:03.199968 Lowest head time:  0 days 00:00:00.033333\n",
            "Total duration:  3.1666350000000003\n",
            "No sway detected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "\n",
        "def organize_data(df):\n",
        "  # Load the label encoder\n",
        "  label_encoder = joblib.load('label_encoder.pkl')\n",
        "  # label_encoder = LabelEncoder()\n",
        "  df['On_or_Off_medication'] = label_encoder.fit_transform(df['On_or_Off_medication'])\n",
        "  df['STS_additional_features'] = label_encoder.fit_transform(df['STS_additional_features'])\n",
        "  df['DBS_state'] = label_encoder.fit_transform(df['DBS_state'])\n",
        "\n",
        "  # Standardize features\n",
        "  scaler = StandardScaler()\n",
        "  X = scaler.fit_transform(df)\n",
        "  return X"
      ],
      "metadata": {
        "id": "YkntInoundbb"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Wjkl_NaSXhvG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce718fbe-7f9c-4ffe-e145-ecdbf894e685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/content/The Joyce Center 3.m4a':\n",
            "  Metadata:\n",
            "    major_brand     : M4A \n",
            "    minor_version   : 0\n",
            "    compatible_brands: M4A isommp42\n",
            "    creation_time   : 2022-09-27T18:40:15.000000Z\n",
            "    title           : The Joyce Center 3\n",
            "    voice-memo-uuid : E052CC62-27EE-469D-BA12-AFBF138DEE22\n",
            "    encoder         : com.apple.VoiceMemos (Katie’s iPhone 13 (null))\n",
            "  Duration: 00:00:07.76, start: 0.000000, bitrate: 70 kb/s\n",
            "  Stream #0:0(und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, mono, fltp, 65 kb/s (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2022-09-27T18:40:15.000000Z\n",
            "      handler_name    : Core Media Audio\n",
            "      vendor_id       : [0][0][0][0]\n",
            "File 'output.mp3' already exists. Overwrite? [y/N] N\n",
            "\u001b[4;31mNot overwriting - exiting\n",
            "\u001b[0mOn or Off medication: Off medication\n",
            "DBS state: -\n",
            "Highest head time:  0 days 00:00:03.199968 Lowest head time:  0 days 00:00:00.033333\n",
            "Total duration:  3.1666350000000003\n",
            "No sway detected.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "sts preds:  [[0.56472415]]\n",
            "voice memo preds:  [[1.]]\n",
            "handwriting preds:  [0.97484004 0.02515993]\n",
            "Weighted Average Predicitions: 0.1816583214327693\n"
          ]
        }
      ],
      "source": [
        "# Load in models and create weighted average\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the PyTorch model\n",
        "# handwriting_model = torch.load('/content/Resnet50_pretrained_weights_43.pth')\n",
        "\n",
        "# Load the Keras models\n",
        "sts_model = load_model('/content/sts_model.h5')\n",
        "voice_memo_model = load_model('/content/voice_memo_model.h5')\n",
        "\n",
        "# Initialize the TFLite interpreter\n",
        "interpreter = tf.lite.Interpreter(model_path=\"/content/model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get voice memo input\n",
        "voice_memo_input = get_voice_memo()\n",
        "\n",
        "# Run sts main to get pose input\n",
        "sts_output_df = main()\n",
        "sts_input = organize_data(sts_output_df)\n",
        "\n",
        "# Get predictions from each model\n",
        "# handwriting_predictions = handwriting_model.predict(handwriting_input)\n",
        "sts_predictions = sts_model.predict(sts_input)\n",
        "voice_memo_predictions = voice_memo_model.predict([voice_memo_input])\n",
        "\n",
        "print('sts preds: ', sts_predictions)\n",
        "print('voice memo preds: ', voice_memo_predictions)\n",
        "print('handwriting preds: ', handwriting_predictions)\n",
        "\n",
        "# Define weights for each model\n",
        "resnet_weight = 0.3\n",
        "sts_weight = 0.4\n",
        "voice_memo_weight = 0.3\n",
        "\n",
        "# Calculate weighted average of predictions\n",
        "# 1 minus prediction of class healthy --> so progression score grows as PD progresses\n",
        "weighted_avg_predictions = (\n",
        "    resnet_weight * handwriting_predictions[1] +\n",
        "    sts_weight * (1 - sts_predictions[0][0]) +\n",
        "    voice_memo_weight * (1 - voice_memo_predictions[0][0])\n",
        ")\n",
        "\n",
        "print(\"Weighted Average Predicitions:\", weighted_avg_predictions)"
      ]
    }
  ]
}